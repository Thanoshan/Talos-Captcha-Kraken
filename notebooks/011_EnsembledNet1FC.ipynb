{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ml8ECsakG1_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.models as models\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2okfMU7uFhV",
        "outputId": "07cac690-0104-4edd-d725-a14073415279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: NVIDIA GeForce RTX 3090 (UUID: GPU-75482a68-87db-0388-ad83-12be720efe26)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGwrTnmnlE25"
      },
      "outputs": [],
      "source": [
        "from scripts.serverside.captcha.image import ImageCaptcha\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import string\n",
        "import random\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeWX_BkQmBCH"
      },
      "outputs": [],
      "source": [
        "def get_random_string(length):\n",
        "    # choose from all lowercase letter\n",
        "    letters = \"abcdefghjkmnpqrstuvwxyz\" + \"ABCDEFGHIJKLMNPQRSTUVWXYZ\" + \"23456789\" + \"     \"\n",
        "    result_str = ''.join(random.choice(letters) for i in range(length))\n",
        "    return result_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIr8rb1jlu4R"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.transforms import Grayscale\n",
        "# Load datasets\n",
        "# Label formated as 2D tensor of indexed as [place][tokenID]\n",
        "chars = \"abcdefghijklmnpqrstuvwxyz123456789 \" # use \" \" as null character. Leave out o and 0, also case insensitive\n",
        "charIndex = {}\n",
        "for i, char in enumerate(chars):\n",
        "    charIndex[char] = i\n",
        "\n",
        "idn = torch.eye(len(chars)).cuda()\n",
        "\n",
        "def strToOH(string):\n",
        "    oneHot = []\n",
        "    for char in string:\n",
        "      oneHot.append(idn[charIndex[char]])\n",
        "    return torch.stack(oneHot)\n",
        "\n",
        "def strToInd(string):\n",
        "    oneHot = []\n",
        "    for char in string:\n",
        "      oneHot.append(torch.tensor(charIndex[char]))\n",
        "    return torch.stack(oneHot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmsfiOEfn7W9"
      },
      "outputs": [],
      "source": [
        "do_cuda = True\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"max_split_size_mb:128\"\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWxJGZ9mnVCg"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, data_loader=None, denoise=False, train=False, batch_size = 64):\n",
        "    if train:\n",
        "        data = captchaPreProcLarge\n",
        "    else:\n",
        "        data = TrawSetVal #TODO: seperate validation set\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if data_loader is None:\n",
        "      data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
        "    for imgs, labels, filtered in data_loader :\n",
        "\n",
        "        if do_cuda and torch.cuda.is_available:\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "            filtered = filtered.cuda()\n",
        "\n",
        "        if denoise:\n",
        "          output = model(filtered)\n",
        "        else:\n",
        "          output = model(imgs, filtered)\n",
        "        for i in range(imgs.shape[0]):\n",
        "            #select index with maximum prediction score\n",
        "            out = output[i, :, :]\n",
        "            #print(out.shape)\n",
        "            #print(digit.shape)\n",
        "            pred = out.max(0, keepdim=True)[1]\n",
        "            pred = pred.squeeze()\n",
        "            #print(pred.shape)\n",
        "            #print(labels[:, i].shape)\n",
        "            #print(labels[i, :].shape)\n",
        "            #print(pred.shape)\n",
        "            correct += int(pred.eq(labels[i, :]).sum().item() == 8)\n",
        "            total += 1\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return correct / total\n",
        "\n",
        "def train(model, data, batch_size=64, num_epochs=30, from_epoch=0, learning_rate=0.0001):\n",
        "  #  train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "    train_loader = None\n",
        "    val_loader = torch.utils.data.DataLoader(TrawSetVal, batch_size=batch_size, shuffle=True)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    iters, losses, train_acc, val_acc = [], [], [], []\n",
        "\n",
        "\n",
        "    startTime = time.time() # to see how long training goes\n",
        "    print(\"starting training\")\n",
        "    # training\n",
        "    n = 0 # the number of iterations\n",
        "    for epoch in range(from_epoch, from_epoch+num_epochs):\n",
        "        \n",
        "        try:\n",
        "            del train_loader\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            data.flush()\n",
        "            train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"epoch: \", epoch)\n",
        "        for imgs, labels, filtered in iter(train_loader):\n",
        "\n",
        "            if do_cuda and torch.cuda.is_available:\n",
        "              imgs = imgs.cuda()\n",
        "              labels = labels.cuda()\n",
        "              filtered = filtered.cuda()\n",
        "\n",
        "            out = model(imgs, filtered)             # forward pass\n",
        "            #print(out[0])\n",
        "            #print()\n",
        "            #print(labels[0])\n",
        "            #print()\n",
        "            loss = criterion(out, labels) # compute the total loss\n",
        "            loss.backward()               # backward pass (compute parameter updates)\n",
        "            optimizer.step()              # make the updates for each parameter\n",
        "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "\n",
        "            # save the current training information\n",
        "            iters.append(n)\n",
        "            losses.append(loss.item()/batch_size)             # compute *average* loss\n",
        "            n += 1\n",
        "\n",
        "        #train_acc.append(get_accuracy(model, train=True, batch_size=batch_size)) # compute training accuracy \n",
        "        val_acc.append(get_accuracy(model, val_loader, train=False, batch_size=batch_size))  # compute validation accuracy\n",
        "        print((\"Epoch {}: |\"+\"Validation acc: {}\").format(\n",
        "                epoch, # call epoch zero epoch zero\n",
        "                \n",
        "                val_acc[-1]))\n",
        "        \n",
        "        #checkpoint\n",
        "        path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(model.name,\n",
        "                                                   batch_size,\n",
        "                                                   learning_rate,\n",
        "                                                   epoch)\n",
        "        torch.save(model.state_dict(), path)\n",
        "\n",
        "\n",
        "    np.savetxt(path+f\"/tacc_{from_epoch+1}_to_{from_epoch+num_epochs}.csv\", \n",
        "                val_acc,\n",
        "                delimiter =\", \", \n",
        "                fmt ='% s')\n",
        "    \n",
        "    finishTime = time.time()\n",
        "\n",
        "    delta = finishTime - startTime\n",
        "    print(\"\\nDONE TRAINING in %s seconds!\\n\" % delta)\n",
        "\n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "    plt.savefig(f\"/loss_e{from_epoch+num_epochs}_{path}.jpg\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Training Curve\")\n",
        "    #plt.plot(range(num_epochs), train_acc, label=\"Train\")\n",
        "    plt.plot(range(num_epochs), val_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Training Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "    plt.savefig(f\"/acc_e{from_epoch+num_epochs}_{path}_{val_acc[-1]}.jpg\")\n",
        "\n",
        "    #print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcUSFkl6Tse1"
      },
      "outputs": [],
      "source": [
        "import run_denoiser, copy\n",
        "class DCacheSet():\n",
        "    def __init__(self, size, period):\n",
        "        self.size = size\n",
        "        self.cached = []\n",
        "        self.period = period\n",
        "        self.IC = ImageCaptcha()\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "       # self.flush()\n",
        "    def flush(self):\n",
        "        self.cached.clear()\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()        \n",
        "        for index in range(self.period):\n",
        "            label = get_random_string(8)\n",
        "            data = self.IC.generate_image(label)\n",
        "            data2 = run_denoiser.execute_pix2pix_denoise(global_denoising, copy.deepcopy(data))\n",
        "            data = self.transform(data)\n",
        "            data2 = self.transform(data2)\n",
        "            label = label.lower().replace(\" \", \"\")\n",
        "            label += \" \" * (8 - len(label))\n",
        "\n",
        "            labelTensor = strToInd(label)\n",
        "            self.cached.append((data, labelTensor, data2))\n",
        "    def __getitem__(self, index):\n",
        "        index = index % self.period\n",
        "        entry = self.cached[index]\n",
        "        return entry[0], entry[1], entry[2]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "class DFrozenSet():\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.cached = []\n",
        "        self.period = size\n",
        "        self.IC = ImageCaptcha()\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        self.flush2()\n",
        "    def flush2(self):\n",
        "        self.cached.clear()\n",
        "        for index in tqdm(range(self.period)):\n",
        "            label = get_random_string(8)\n",
        "            data = self.IC.generate_image(label)\n",
        "            data2 = run_denoiser.execute_pix2pix_denoise(global_denoising, copy.deepcopy(data))\n",
        "            data = self.transform(data)\n",
        "            data2 = self.transform(data2)\n",
        "            label = label.lower().replace(\" \", \"\")\n",
        "            label += \" \" * (8 - len(label))\n",
        "\n",
        "            labelTensor = strToInd(label)\n",
        "            self.cached.append((data, labelTensor, data2))\n",
        "    def __getitem__(self, index):\n",
        "        index = index % self.period\n",
        "        entry = self.cached[index]\n",
        "        return entry[0], entry[1], entry[2]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6WHFggG6Tyx"
      },
      "outputs": [],
      "source": [
        "class TCacheSet():\n",
        "    def __init__(self, size, period):\n",
        "        self.size = size\n",
        "        self.cached = []\n",
        "        self.period = period\n",
        "        self.IC = ImageCaptcha()\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        self.flush()\n",
        "    def flush(self):\n",
        "        self.cached.clear()\n",
        "        for index in range(self.period):\n",
        "            label = get_random_string(8)\n",
        "            data = self.IC.generate_image(label)\n",
        "            data = self.transform(data)\n",
        "            label = label.lower().replace(\" \", \"\")\n",
        "            label += \" \" * (8 - len(label))\n",
        "\n",
        "            labelTensor = strToInd(label)\n",
        "            self.cached.append((data, labelTensor))\n",
        "    def __getitem__(self, index):\n",
        "        index = index % self.period\n",
        "        entry = self.cached[index]\n",
        "        return entry[0], entry[1], 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "class TFrozenSet():\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.cached = []\n",
        "        self.period = size\n",
        "        self.IC = ImageCaptcha()\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        self.flush2()\n",
        "    def flush2(self):\n",
        "        self.cached.clear()\n",
        "        for index in range(self.period):\n",
        "            label = get_random_string(8)\n",
        "            data = self.IC.generate_image(label)\n",
        "            data = self.transform(data)\n",
        "            label = label.lower().replace(\" \", \"\")\n",
        "            label += \" \" * (8 - len(label))\n",
        "\n",
        "            labelTensor = strToInd(label)\n",
        "            self.cached.append((data, labelTensor))\n",
        "    def __getitem__(self, index):\n",
        "        index = index % self.period\n",
        "        entry = self.cached[index]\n",
        "        return entry[0], entry[1], 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "class TransferCaptcha(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransferCaptcha, self).__init__()\n",
        "        self.name = \"TransferCaptcha\"\n",
        "        conv = models.resnext50_32x4d(pretrained=True)\n",
        "        self.conv = conv\n",
        "        self.layer1 = nn.Linear(1000, 512)\n",
        "        self.layer2 = nn.Linear(512, len(chars) * 8)\n",
        "      \n",
        "    def forward(self, x, x2=0):\n",
        "        \n",
        "        x = self.conv(x)\n",
        "\n",
        "        x = x.view(-1, 1000) # flatten convolution output for ANN\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        x = x.view(-1, len(chars), 8)\n",
        "        #x = x.squeeze(1) # Flatten to [batch_size]\n",
        "        return x  \n",
        "\n",
        "class ViTCaptcha(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ViTCaptcha, self).__init__()\n",
        "        self.name = \"TransferCaptchaViT\"\n",
        "        self.enc = models.vit_b_32(pretrained=True)\n",
        "        self.layer1 = nn.Linear(1000, len(chars) * 8)\n",
        "      \n",
        "    def forward(self, x, x2=0):\n",
        "        \n",
        "        x = self.enc(x)\n",
        "\n",
        "        x = x.view(-1, 1000) # flatten enc output for ANN\n",
        "        x = self.layer1(F.relu(x))\n",
        "        x = x.view(-1, len(chars), 8)\n",
        "        #x = x.squeeze(1) # Flatten to [batch_size]\n",
        "        return x  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5Xsjw-cYS-j"
      },
      "outputs": [],
      "source": [
        "class ECaptcha2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ECaptcha2, self).__init__()\n",
        "        self.name = \"ETransferCaptcha2\"\n",
        "        self.VIT = ViTCaptcha()\n",
        "        self.CONV = TransferCaptcha()\n",
        "        self.VIT2 = ViTCaptcha()\n",
        "        self.CONV2 = TransferCaptcha()\n",
        "        self.VIT.requires_grad = False\n",
        "        self.CONV.requires_grad = False\n",
        "        self.VIT2.requires_grad = False\n",
        "        self.CONV2.requires_grad = False\n",
        "\n",
        "        self.ensembler = nn.Linear(len(chars) * 8 * 4, len(chars) * 8)\n",
        "    def forward(self, x, x2):\n",
        "        with torch.no_grad():\n",
        "          pre1 = self.VIT(x.clone()).view(-1, len(chars) * 8)\n",
        "          pre2 = self.CONV(x.clone()).view(-1, len(chars) * 8)\n",
        "\n",
        "          pre3 = self.VIT2(x2.clone()).view(-1, len(chars) * 8)\n",
        "          pre4 = self.CONV2(x2.clone()).view(-1, len(chars) * 8)\n",
        "        x = torch.cat((pre1, pre2, pre3, pre4), 1)\n",
        "        x = self.ensembler(x)\n",
        "        x = x.view(-1, len(chars), 8)\n",
        "        #x = x.squeeze(1) # Flatten to [batch_size]\n",
        "        return x  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8gkWa8pYzYy"
      },
      "outputs": [],
      "source": [
        "model2 = ECaptcha2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWLDztPaY7o1",
        "outputId": "e2764c27-5020-4901-d9a8-604438ebe8dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "base = \"checkpoints/\"\n",
        "model2.VIT2.load_state_dict(torch.load(base+\"Denoised_ViT_V1_955Acc\"))\n",
        "model2.VIT.load_state_dict(torch.load(base+\"model_TransferCaptchaViT_bs64_lr3.1e-07_epoch835\"))\n",
        "model2.CONV2.load_state_dict(torch.load(base+\"Denoised_ResNet_941Acc\"))\n",
        "model2.CONV.load_state_dict(torch.load(base+\"model_TransferCaptcha_bs64_lr2.11231203e-05_epoch27\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZK9fKH1ZUW6",
        "outputId": "c7b7085d-f65b-4867-abf5-483ae403ca8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ECaptcha2(\n",
              "  (VIT): ViTCaptcha(\n",
              "    (enc): VisionTransformer(\n",
              "      (conv_proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
              "      (encoder): Encoder(\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (layers): Sequential(\n",
              "          (encoder_layer_0): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_1): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_2): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_3): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_4): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_5): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_6): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_7): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_8): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_9): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_10): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_11): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (heads): Sequential(\n",
              "        (head): Linear(in_features=768, out_features=1000, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (layer1): Linear(in_features=1000, out_features=280, bias=True)\n",
              "  )\n",
              "  (CONV): TransferCaptcha(\n",
              "    (conv): ResNet(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "      (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              "    )\n",
              "    (layer1): Linear(in_features=1000, out_features=512, bias=True)\n",
              "    (layer2): Linear(in_features=512, out_features=280, bias=True)\n",
              "  )\n",
              "  (VIT2): ViTCaptcha(\n",
              "    (enc): VisionTransformer(\n",
              "      (conv_proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
              "      (encoder): Encoder(\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (layers): Sequential(\n",
              "          (encoder_layer_0): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_1): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_2): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_3): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_4): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_5): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_6): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_7): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_8): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_9): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_10): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (encoder_layer_11): EncoderBlock(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attention): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): MLPBlock(\n",
              "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (act): GELU()\n",
              "              (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout_2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (heads): Sequential(\n",
              "        (head): Linear(in_features=768, out_features=1000, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (layer1): Linear(in_features=1000, out_features=280, bias=True)\n",
              "  )\n",
              "  (CONV2): TransferCaptcha(\n",
              "    (conv): ResNet(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "      (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              "    )\n",
              "    (layer1): Linear(in_features=1000, out_features=512, bias=True)\n",
              "    (layer2): Linear(in_features=512, out_features=280, bias=True)\n",
              "  )\n",
              "  (ensembler): Linear(in_features=1120, out_features=280, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model2.VIT.requires_grad = False\n",
        "model2.CONV.requires_grad = False\n",
        "model2.VIT2.requires_grad = False\n",
        "model2.CONV2.requires_grad = False\n",
        "model2.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVmU58JgNLoH",
        "outputId": "26fe8a9e-3d9c-4af3-ec36-30e694eabffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initialize network with normal\n"
          ]
        }
      ],
      "source": [
        "import run_denoiser\n",
        "global_denoising = run_denoiser.load_pix2pix_CAPTCHA()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1uBIl27-Vqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0415571c-dd69-4bb9-8fa5-deca4162b122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 250/250 [00:08<00:00, 29.10it/s]\n"
          ]
        }
      ],
      "source": [
        "random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "TrawSetVal = DFrozenSet(250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJXPTxg5qAjZ"
      },
      "outputs": [],
      "source": [
        "TbiggerSet = DCacheSet(1024, 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxugMK5bGkFj",
        "outputId": "db9e38f2-636d-4da2-eee5-970bc5883778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONV ACC: 0.56\n",
            "VIT ACC: 0.588\n",
            "DENOISED CONV ACC: 0.692\n",
            "DENOISED VIT ACC: 0.752\n"
          ]
        }
      ],
      "source": [
        "print(\"CONV ACC:\",  get_accuracy(model2.CONV))\n",
        "print(\"VIT ACC:\",  get_accuracy(model2.VIT))\n",
        "print(\"DENOISED CONV ACC:\",  get_accuracy(model2.CONV2, denoise=True))\n",
        "print(\"DENOISED VIT ACC:\",  get_accuracy(model2.VIT2, denoise=True))\n",
        "print(\"FULL ENSEMBLE ACC:\",  get_accuracy(model2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIFiMEcMLDA9",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f46ea7ea-f34b-4df1-f971-d025ca2f7342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting training\n",
            "epoch:  171\n",
            "Epoch 171: |Validation acc: 0.808\n",
            "epoch:  172\n",
            "Epoch 172: |Validation acc: 0.816\n",
            "epoch:  173\n",
            "Epoch 173: |Validation acc: 0.828\n",
            "epoch:  174\n",
            "Epoch 174: |Validation acc: 0.844\n",
            "epoch:  175\n",
            "Epoch 175: |Validation acc: 0.82\n",
            "epoch:  176\n",
            "Epoch 176: |Validation acc: 0.844\n",
            "epoch:  177\n",
            "Epoch 177: |Validation acc: 0.836\n",
            "epoch:  178\n",
            "Epoch 178: |Validation acc: 0.836\n",
            "epoch:  179\n",
            "Epoch 179: |Validation acc: 0.828\n",
            "epoch:  180\n",
            "Epoch 180: |Validation acc: 0.836\n",
            "epoch:  181\n",
            "Epoch 181: |Validation acc: 0.836\n",
            "epoch:  182\n",
            "Epoch 182: |Validation acc: 0.852\n",
            "epoch:  183\n",
            "Epoch 183: |Validation acc: 0.852\n",
            "epoch:  184\n",
            "Epoch 184: |Validation acc: 0.832\n",
            "epoch:  185\n",
            "Epoch 185: |Validation acc: 0.836\n",
            "epoch:  186\n",
            "Epoch 186: |Validation acc: 0.852\n",
            "epoch:  187\n",
            "Epoch 187: |Validation acc: 0.852\n",
            "epoch:  188\n",
            "Epoch 188: |Validation acc: 0.852\n",
            "epoch:  189\n",
            "Epoch 189: |Validation acc: 0.844\n",
            "epoch:  190\n",
            "Epoch 190: |Validation acc: 0.856\n",
            "epoch:  191\n",
            "Epoch 191: |Validation acc: 0.844\n",
            "epoch:  192\n",
            "Epoch 192: |Validation acc: 0.848\n",
            "epoch:  193\n",
            "Epoch 193: |Validation acc: 0.852\n",
            "epoch:  194\n",
            "Epoch 194: |Validation acc: 0.836\n",
            "epoch:  195\n",
            "Epoch 195: |Validation acc: 0.848\n",
            "epoch:  196\n",
            "Epoch 196: |Validation acc: 0.852\n",
            "epoch:  197\n",
            "Epoch 197: |Validation acc: 0.848\n",
            "epoch:  198\n",
            "Epoch 198: |Validation acc: 0.836\n",
            "epoch:  199\n",
            "Epoch 199: |Validation acc: 0.852\n",
            "epoch:  200\n",
            "Epoch 200: |Validation acc: 0.844\n",
            "epoch:  201\n",
            "Epoch 201: |Validation acc: 0.84\n",
            "epoch:  202\n",
            "Epoch 202: |Validation acc: 0.852\n",
            "epoch:  203\n",
            "Epoch 203: |Validation acc: 0.844\n",
            "epoch:  204\n",
            "Epoch 204: |Validation acc: 0.844\n",
            "epoch:  205\n",
            "Epoch 205: |Validation acc: 0.86\n",
            "epoch:  206\n",
            "Epoch 206: |Validation acc: 0.844\n",
            "epoch:  207\n",
            "Epoch 207: |Validation acc: 0.848\n",
            "epoch:  208\n",
            "Epoch 208: |Validation acc: 0.844\n",
            "epoch:  209\n",
            "Epoch 209: |Validation acc: 0.836\n",
            "epoch:  210\n",
            "Epoch 210: |Validation acc: 0.836\n",
            "epoch:  211\n",
            "Epoch 211: |Validation acc: 0.832\n",
            "epoch:  212\n",
            "Epoch 212: |Validation acc: 0.836\n",
            "epoch:  213\n",
            "Epoch 213: |Validation acc: 0.86\n",
            "epoch:  214\n",
            "Epoch 214: |Validation acc: 0.852\n",
            "epoch:  215\n",
            "Epoch 215: |Validation acc: 0.852\n",
            "epoch:  216\n",
            "Epoch 216: |Validation acc: 0.852\n",
            "epoch:  217\n",
            "Epoch 217: |Validation acc: 0.84\n",
            "epoch:  218\n",
            "Epoch 218: |Validation acc: 0.848\n",
            "epoch:  219\n",
            "Epoch 219: |Validation acc: 0.836\n",
            "epoch:  220\n",
            "Epoch 220: |Validation acc: 0.84\n",
            "epoch:  221\n",
            "Epoch 221: |Validation acc: 0.832\n",
            "epoch:  222\n",
            "Epoch 222: |Validation acc: 0.816\n",
            "epoch:  223\n",
            "Epoch 223: |Validation acc: 0.832\n",
            "epoch:  224\n",
            "Epoch 224: |Validation acc: 0.836\n",
            "epoch:  225\n",
            "Epoch 225: |Validation acc: 0.844\n",
            "epoch:  226\n",
            "Epoch 226: |Validation acc: 0.856\n",
            "epoch:  227\n",
            "Epoch 227: |Validation acc: 0.852\n",
            "epoch:  228\n",
            "Epoch 228: |Validation acc: 0.844\n",
            "epoch:  229\n",
            "Epoch 229: |Validation acc: 0.852\n",
            "epoch:  230\n",
            "Epoch 230: |Validation acc: 0.844\n",
            "epoch:  231\n",
            "Epoch 231: |Validation acc: 0.84\n",
            "epoch:  232\n",
            "Epoch 232: |Validation acc: 0.844\n",
            "epoch:  233\n",
            "Epoch 233: |Validation acc: 0.852\n",
            "epoch:  234\n",
            "Epoch 234: |Validation acc: 0.844\n",
            "epoch:  235\n",
            "Epoch 235: |Validation acc: 0.844\n",
            "epoch:  236\n",
            "Epoch 236: |Validation acc: 0.844\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:380\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 380\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:604\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    603\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[1;32m--> 604\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m model2\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_ETransferCaptcha2_bs128_lr0.0005_epoch200_836\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      2\u001b[0m model2\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTbiggerSet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m171\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data, batch_size, num_epochs, from_epoch, learning_rate)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m#checkpoint\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m_bs\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m_lr\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m_epoch\u001b[39m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m     95\u001b[0m                                                batch_size,\n\u001b[0;32m     96\u001b[0m                                                learning_rate,\n\u001b[0;32m     97\u001b[0m                                                epoch)\n\u001b[1;32m---> 98\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tacc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfrom_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_to_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfrom_epoch\u001b[38;5;241m+\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m    102\u001b[0m             val_acc,\n\u001b[0;32m    103\u001b[0m             delimiter \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m    104\u001b[0m             fmt \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m% s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    106\u001b[0m finishTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:379\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 379\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    380\u001b[0m             _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:260\u001b[0m, in \u001b[0;36m_open_zipfile_writer_buffer.__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 260\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mflush()\n",
            "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\caffe2\\serialize\\inline_container.cc:300] . unexpected pos 410166080 vs 410165968"
          ]
        }
      ],
      "source": [
        "model2.load_state_dict(torch.load(\"model_ETransferCaptcha2_bs128_lr0.0005_epoch200_836\"))\n",
        "model2.to(\"cuda\")\n",
        "train(model2, TbiggerSet, batch_size=128, num_epochs=200, from_epoch=171, learning_rate=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGyndX2lB77Z"
      },
      "source": [
        "## Memory Diagnosis code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_G5NtMixqB5"
      },
      "outputs": [],
      "source": [
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "        profile_memory=True, record_shapes=True) as prof:\n",
        "  try:\n",
        "    train(model2, TbiggerSet, batch_size=64, num_epochs=200, learning_rate=0.001)\n",
        "  except RuntimeError:\n",
        "    print(\":(\")\n",
        "  import gc; gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prof.export_chrome_trace(\"trace.json\")\n",
        "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
      ],
      "metadata": {
        "id": "wcgqWAl9-V3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO-q3rg69jBr"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mptd8KSqJ_q7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.max_memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elnFkKNw-nx5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.memory_summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5R3l60MJUph"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "010_EnsembledNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}